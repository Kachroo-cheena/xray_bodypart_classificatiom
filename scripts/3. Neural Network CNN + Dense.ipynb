{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><strong> X-Ray Body Parts Prediction </center></strong></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-19 15:41:26.001407: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-19 15:41:26.012497: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-19 15:41:26.156178: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-19 15:41:26.157773: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-19 15:41:26.910902: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-19 15:41:27.517971: W itex/core/wrapper/itex_gpu_wrapper.cc:32] Could not load dynamic library: libmkl_sycl.so.3: cannot open shared object file: No such file or directory\n",
      "2023-08-19 15:41:27.584169: I itex/core/wrapper/itex_cpu_wrapper.cc:42] Intel Extension for Tensorflow* AVX512 CPU backend is loaded.\n",
      "2023-08-19 15:41:27.620364: W itex/core/ops/op_init.cc:58] Op: _QuantizedMaxPool3D is already registered in Tensorflow\n",
      "2023-08-19 15:41:27.632305: E itex/core/wrapper/itex_gpu_wrapper.cc:49] Could not load Intel Extension for Tensorflow* GPU backend, GPU will not be used.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n",
      "2023-08-19 15:41:27.633193: E itex/core/wrapper/itex_gpu_wrapper.cc:49] Could not load Intel Extension for Tensorflow* GPU backend, GPU will not be used.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, InputLayer\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'images')\n",
    "\n",
    "MIXED_IMAGES = os.path.join(IMAGES_DIR, 'mixed')\n",
    "WRIST_IMAGES = os.path.join(IMAGES_DIR, 'wrist')\n",
    "LUMBER_SPINE_IMAGES = os.path.join(IMAGES_DIR, 'lumber_spine')\n",
    "PELVIS_IMAGES = os.path.join(IMAGES_DIR, 'pelvis')\n",
    "CHEST_IMAGES = os.path.join(IMAGES_DIR, 'chest')\n",
    "HAND_IMAGES = os.path.join(IMAGES_DIR, 'hand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load images and labels from multiple directories into one DF\n",
    "Steps (load_df):\n",
    "    - Mixed\n",
    "        - load df from csv\n",
    "        - drop extra columns other than SOPInstance and Target\n",
    "        - Change SOPInstance coloumn to 'Image name'\n",
    "    - Lumber_spine\n",
    "        - listdir() for all images in the directory\n",
    "        - make a df with two columns 'Image name' and 'Target'\n",
    "        - populate 'Image name' from listdir() and 'Target' to list interger as 14\n",
    "    - Wrist\n",
    "        - listdir() for all images in the directory\n",
    "        - make a df with two columns 'Image name' and 'Target'\n",
    "        - populate 'Image name' from listdir() and 'Target' to list interger as 21\n",
    "    - Finally: Merge (Mixed, Lumber_spine and Wrist)\n",
    "        - Merge all three df on axis=1 'Image name'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10025629581362719970...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10036150326276641158...</td>\n",
       "      <td>[15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10038426859954986240...</td>\n",
       "      <td>[12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10050991192143676483...</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10053309524595490852...</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10053755320637729867...</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10062189329714053601...</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10065930002825553435...</td>\n",
       "      <td>[13, 20]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Instance    Target\n",
       "0  1.2.826.0.1.3680043.8.498.10025629581362719970...       [0]\n",
       "1  1.2.826.0.1.3680043.8.498.10036150326276641158...      [15]\n",
       "2  1.2.826.0.1.3680043.8.498.10038426859954986240...      [12]\n",
       "3  1.2.826.0.1.3680043.8.498.10050991192143676483...      [14]\n",
       "4  1.2.826.0.1.3680043.8.498.10053309524595490852...       [3]\n",
       "5  1.2.826.0.1.3680043.8.498.10053755320637729867...       [3]\n",
       "6  1.2.826.0.1.3680043.8.498.10062189329714053601...      [14]\n",
       "7  1.2.826.0.1.3680043.8.498.10065930002825553435...  [13, 20]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing dataframe from 'mixed' directory \n",
    "mixed_df = pd.read_csv(os.path.join(os.getcwd(), 'data', 'train_df.csv'))\n",
    "\n",
    "mixed_df = mixed_df[['SOPInstanceUID', 'Target']]\n",
    "mixed_df = mixed_df.rename(columns={'SOPInstanceUID':'Instance'})\n",
    "\n",
    "mixed_df['Target'] = mixed_df['Target'].apply(lambda x: x.strip())\n",
    "mixed_df['Target'] = mixed_df['Target'].apply(lambda x: x.split(' '))\n",
    "\n",
    "mixed_df['Instance'] = mixed_df['Instance'].apply(lambda x: x + '-c.png')\n",
    "\n",
    "mixed_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imgtr33705.png</td>\n",
       "      <td>[9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>imgtr33905.png</td>\n",
       "      <td>[9]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Instance Target\n",
       "0  imgtr33705.png    [9]\n",
       "1  imgtr33905.png    [9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing dataframe from 'Hand' directory\n",
    "hand_images = os.listdir(HAND_IMAGES)\n",
    "hand_images = [image_name for image_name in hand_images if image_name.lower().endswith('.png')]\n",
    "hand_target = [[int(9)] for image_name in hand_images]\n",
    "\n",
    "hand_df = pd.DataFrame({'Instance': hand_images, 'Target': hand_target})\n",
    "hand_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IM-0595-0001.jpeg</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NORMAL2-IM-0775-0001.jpeg</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Instance Target\n",
       "0          IM-0595-0001.jpeg    [3]\n",
       "1  NORMAL2-IM-0775-0001.jpeg    [3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing dataframe from 'Chest' directory\n",
    "chest_images = os.listdir(CHEST_IMAGES)\n",
    "chest_images = [image_name for image_name in chest_images if image_name.lower().endswith('.jpeg')]\n",
    "chest_target = [[int(3)] for image_name in chest_images]\n",
    "\n",
    "chest_df = pd.DataFrame({'Instance': chest_images, 'Target': chest_target})\n",
    "chest_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pelvicfx2267.png</td>\n",
       "      <td>[15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pelvicfx3459.png</td>\n",
       "      <td>[15]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Instance Target\n",
       "0  pelvicfx2267.png   [15]\n",
       "1  pelvicfx3459.png   [15]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing dataframe from 'Pelvis' directory\n",
    "pelvis_images = os.listdir(PELVIS_IMAGES)\n",
    "pelvis_images = [image_name for image_name in pelvis_images if image_name.lower().endswith('.png')]\n",
    "pelvis_target = [[int(15)] for image_name in pelvis_images]\n",
    "\n",
    "pelvis_df = pd.DataFrame({'Instance': pelvis_images, 'Target': pelvis_target})\n",
    "pelvis_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0123_0352338282_01_WRI-R2_M004.png</td>\n",
       "      <td>[21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0842_1090707732_01_WRI-R3_M008.png</td>\n",
       "      <td>[21]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Instance Target\n",
       "0  0123_0352338282_01_WRI-R2_M004.png   [21]\n",
       "1  0842_1090707732_01_WRI-R3_M008.png   [21]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing dataframe from 'Wrist' directory\n",
    "wrist_images = os.listdir(WRIST_IMAGES)\n",
    "wrist_images = [image_name for image_name in wrist_images if image_name.lower().endswith('.png')]\n",
    "wrist_target = [[int(21)] for image_name in wrist_images]\n",
    "\n",
    "wrist_df = pd.DataFrame({'Instance': wrist_images, 'Target': wrist_target})\n",
    "wrist_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge all image df\n",
    "df_all = pd.concat([mixed_df, pelvis_df, wrist_df, chest_df])\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels(label):\n",
    "    if label == 0:\n",
    "        return 'Abdomen'\n",
    "    elif label == 1:\n",
    "        return 'Ankle'\n",
    "    elif label == 2:\n",
    "        return 'Cervical Spine'\n",
    "    elif label == 3:\n",
    "        return 'Chest'\n",
    "    elif label == 4:\n",
    "        return 'Clavicles'\n",
    "    elif label == 5:\n",
    "        return 'Elbow'\n",
    "    elif label == 6:\n",
    "        return 'Feet'\n",
    "    elif label == 7:\n",
    "        return 'Finger'\n",
    "    elif label == 8:\n",
    "        return 'Forearm'\n",
    "    elif label == 9:\n",
    "        return 'Hand'\n",
    "    elif label == 10:\n",
    "        return 'Hip'\n",
    "    elif label == 11:\n",
    "        return 'Knee'\n",
    "    elif label == 12:\n",
    "        return 'Lower Leg'\n",
    "    elif label == 13:\n",
    "        return 'Lumbar Spine'\n",
    "    elif label == 14:\n",
    "        return 'Others'\n",
    "    elif label == 15:\n",
    "        return 'Pelvis'\n",
    "    elif label == 16:\n",
    "        return 'Shoulder'\n",
    "    elif label == 17:\n",
    "        return 'Sinus'\n",
    "    elif label == 18:\n",
    "        return 'Skull'\n",
    "    elif label == 19:\n",
    "        return 'Thigh'\n",
    "    elif label == 20:\n",
    "        return 'Thoracic Spine'\n",
    "    elif label == 21:\n",
    "        return 'Wrist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  12  13  14  15  16  17  18  \\\n",
       "0   1   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "1   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   1   0   0   0   \n",
       "\n",
       "   19  20  21  \n",
       "0   0   0   0  \n",
       "1   0   0   0  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for multiclassfication problem using MultiLabelBinarizer()\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# transform labels into multilabelbinary format\n",
    "encoded_labels = mlb.fit_transform(df_all['Target'].apply(lambda x : [int(label) for label in x]))\n",
    "encoded_labels_df = pd.DataFrame(encoded_labels, columns=mlb.classes_)\n",
    "encoded_labels_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 22)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10025629581362719970...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10036150326276641158...</td>\n",
       "      <td>[15]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10038426859954986240...</td>\n",
       "      <td>[12]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10050991192143676483...</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.2.826.0.1.3680043.8.498.10053309524595490852...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Instance Target  0  1  2  3  4  5  \\\n",
       "0  1.2.826.0.1.3680043.8.498.10025629581362719970...    [0]  1  0  0  0  0  0   \n",
       "1  1.2.826.0.1.3680043.8.498.10036150326276641158...   [15]  0  0  0  0  0  0   \n",
       "2  1.2.826.0.1.3680043.8.498.10038426859954986240...   [12]  0  0  0  0  0  0   \n",
       "3  1.2.826.0.1.3680043.8.498.10050991192143676483...   [14]  0  0  0  0  0  0   \n",
       "4  1.2.826.0.1.3680043.8.498.10053309524595490852...    [3]  0  0  0  1  0  0   \n",
       "\n",
       "   6  7  ...  12  13  14  15  16  17  18  19  20  21  \n",
       "0  0  0  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "1  0  0  ...   0   0   0   1   0   0   0   0   0   0  \n",
       "2  0  0  ...   1   0   0   0   0   0   0   0   0   0  \n",
       "3  0  0  ...   0   0   1   0   0   0   0   0   0   0  \n",
       "4  0  0  ...   0   0   0   0   0   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset index and concat the dataframe with Instance and its MultiLabelBinaries\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "encoded_labels_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = pd.concat([df_all, encoded_labels_df], axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Abdomen 92\n",
      "1 Ankle 54\n",
      "2 Cervical Spine 81\n",
      "3 Chest 2079\n",
      "4 Clavicles 9\n",
      "5 Elbow 25\n",
      "6 Feet 78\n",
      "7 Finger 17\n",
      "8 Forearm 15\n",
      "9 Hand 74\n",
      "10 Hip 25\n",
      "11 Knee 110\n",
      "12 Lower Leg 26\n",
      "13 Lumbar Spine 87\n",
      "14 Others 120\n",
      "15 Pelvis 221\n",
      "16 Shoulder 41\n",
      "17 Sinus 23\n",
      "18 Skull 10\n",
      "19 Thigh 15\n",
      "20 Thoracic Spine 64\n",
      "21 Wrist 3276\n"
     ]
    }
   ],
   "source": [
    "for idx, column in enumerate(df.iloc[:,2:].columns, start=0):\n",
    "    print(idx, labels(idx), df.iloc[:,2+idx:2+idx+1].sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images have different sizes we need to standardize them to (128, 128) pixels for this we will use cv2.resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images = []\n",
    "Y_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# image preprocessing\n",
    "for i, val in enumerate(df.Instance):\n",
    "    mixed_images = [image for image in os.listdir(MIXED_IMAGES) if image.lower().endswith(('.png', 'jpeg'))]\n",
    "    pelvis_images = [image for image in os.listdir(PELVIS_IMAGES) if image.lower().endswith(('.png', 'jpeg'))]\n",
    "    wrist_images = [image for image in os.listdir(WRIST_IMAGES) if image.lower().endswith(('.png', 'jpeg'))]\n",
    "    chest_images = [image for image in os.listdir(CHEST_IMAGES) if image.lower().endswith(('.png', 'jpeg'))]\n",
    "    hand_images = [image for image in os.listdir(HAND_IMAGES) if image.lower().endswith(('.png', 'jpeg'))]\n",
    "\n",
    "    if val in mixed_images:\n",
    "        image_path = os.path.join(MIXED_IMAGES, val)\n",
    "    elif val in pelvis_images:\n",
    "        image_path = os.path.join(PELVIS_IMAGES, val)\n",
    "    elif val in wrist_images:\n",
    "        image_path = os.path.join(WRIST_IMAGES, val)\n",
    "    elif val in chest_images:\n",
    "        image_path = os.path.join(CHEST_IMAGES, val)\n",
    "    elif val in hand_images:\n",
    "        image_path = os.path.join(HAND_IMAGES, val)\n",
    "    \n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resize = cv2.resize(img, dsize=(128, 128))\n",
    "    img_norm = cv2.normalize(img_resize, None, alpha=0, beta=500, norm_type=cv2.NORM_MINMAX)\n",
    "    # var 'X' will contain the array representation of orginal image and we add a new dimention to the array at start. For CNN we need the data in this shape batch_size, height, width, channels)\n",
    "    X_images.append(np.expand_dims(img_norm, axis=-1))\n",
    "    # flatten img_norm into 1d array and store it in list var 'Y'\n",
    "    #Y_labels.append(np.ndarray.flatten(np.array(new_train_df.loc[i][new_train_df.columns[65:]])))\n",
    "\n",
    "X_images = np.array(X_images)\n",
    "Y_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_images_train, x_images_test, y_labels_train, y_labels_test = train_test_split(X_images, Y_labels, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5440, 128, 128, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_images_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5440, 22)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your data and preprocess it, including converting grayscale images to RGB (for ResNet Layer)\n",
    "x_images_train_rgb = np.repeat(x_images_train, 3, axis=-1)\n",
    "x_images_test_rgb = np.repeat(x_images_test, 3, axis=-1)\n",
    "\n",
    "# Define the input shape\n",
    "x_images_train_rgb[0].shape  # Use the shape of one image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN MODEL WITH MULTI-LAYERS\n",
    "Using Convolutional Neural Network (CNN) and Keras API to desgin an image classification neutral network.\n",
    "\n",
    "- 'Sequential' = is used to create a linear stack of layers\n",
    "        model = tf.keras.models.Sequential()\n",
    "        Sequential is the fundamental archetecture of neural networks. Think it as pipeline where data flows sequentially through each layer in the model. The output of one layer will become the input of the next layer. \n",
    "\n",
    "- 'InputLayer' = is defined to specify the input shape (128,128,1) 128x128 pixels and 1 channel for grayscale\n",
    "        model.add(InputLayer(input_shape=(128,128,1)))\n",
    "        While initialling the neural network especially Sequential model you should define the input shape of your data because:\n",
    "        - This will initalize the weights and biases of the sebsequent layers correctly. This is needed so the network setup connections between the layers and allocate appropiate number of parameters for each layer\n",
    "        - Specifing the input shape helps catch errors and inconsistancies earlier in the building process. If you try to connect a layer with the wrong input shape the framework will raise an error making it easier to debug and troubleshoot your model.\n",
    "\n",
    "- 'Conv2D' = (CNN) layer is added for feature extraction\n",
    "        model.add(Conv2D(64, kernal_size=(3,3), activation='relu', kernal_initializer='he_normal', padding='same'))\n",
    "        Conv2D(CNN) parameters include \n",
    "        - '64' filters (kernals/channels): means in CNN each filter learns to detect different features in the input data \n",
    "        - 'kernal_size: this defines the filter size that slides over the input data meaning the (3x3) pixel filter will slide over the image to detect features\n",
    "        - 'activation': this function help the model by allowing it to generate a non-linear output for each element in the image array. 'relu' (Rectified Linear Activation) allows non-linearity while being computationally efficent.\n",
    "        - 'kenal_initialization': this defines the weight initialization techniques for filter weights. 'he_normal' is an initailization method that helps prevent vanishing gradients during training.\n",
    "        - 'padding': this defines the padding strategy for input data before CNN operation. 'same' padding means that the input data will be padded with zeros in such a way that the output has the same spatial dimentions as the input. padding helps maintain the spatial dimensions of the input and output volumes after convolution.  \n",
    "\n",
    "- 'MaxPooling2D' = (max pooling) layer is added to downsize the spatial dimensions to reduce computations\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        'MaxPooling2D' is used in CNN to downsize the spatial dimentions (width, height) of the input data while retaining important information. This is done by retaining the maximum value from a group of neighbouring pixels in the input hence reducing the dimention of the data. '(2x2)' means that this operation takes a maximum value within each 2x2 window\n",
    "\n",
    "- 'Dropout' = layer is added for regularization, which helps prevent overfitting by randomly setting fraction inputs to 0 during each update\n",
    "        model.add(Dropout(0.6))\n",
    "        Dropout layer is used to prevent overfitting in a neural network. Overfitting occurs when a model learn extremely well on the trained data but fails to generalize to new unseen data. Dropout helps improve model generalization by introducing randomness during training. At each training layer a different set of neurons is dropped out which means they are not used to compute the output of the layer. The dropout rate is the percentage of neurons that are dropped out. In this example 60% neurons in EACH layer will be randomly dropped out during training. The remaining 40% neurons will be used to compute the output of the layer.\n",
    "\n",
    "- 'Flatten' = layer flattens the output from Conv2D into a vector (1-D vector) so that it can be processed by Dense Layers (fully connected layers)\n",
    "        model.add(Flatten())\n",
    "        Flatten layer reshapes the input data to one-dimentional array. This is placed at the end of the sequence of CNN layers just before the fully-connected layer Dense() in a neural network. The purpose is to flatten the input that can be fed into Dense() layer. This conversion is required because Dense() layers expect one-dimentional array as they are fully connected to establish network. For example if the CNN produces a feature map (batch_size, height, width, channels), the flatten size is the product of (height x weigth x channel) \n",
    "\n",
    "- 'Dense' = represent fully connected layers. These layers process the flattened features to make decisions. Layer as many units as classes in your classification problem (41 in this case). This uses the activation fucntion to output class probabilities \n",
    "        model.add(Dense(2048, activation='relu'))\n",
    "        model.add(Dense(41, activation='softmax'))\n",
    "        Dense layers are fully-connected layers they take input from CNN earlier layers extracted features to learn complex relationships in the data. Dense() takes in number of neurons(units) as a parameter in the Dense() layer. Each neuron will learn to capture differnet patterns and features from the input data to final class scores prediction in a classification problem. \n",
    "        - activation function 'relu' (Rectification Linear Activation) applies to the output of the neurons in the Dense() layer. ReLU is a popular choice for acitvation function because its non-linear while being computationally efficent.\n",
    "        - activation function 'softmax' is usually used in the output layer of the neural networks, especially for multi-class classifications problems. The output of the softmax function defines the probalities that the input belongs to each class\n",
    "\n",
    "- 'model.compile' = the model is compiled using 'Adam' optimizer assuming binary (0 to 1) classification and accuracy as the evaluation metric\n",
    "        model.compile(optimizer='Adam', loss='BinaryCrossentropy', metrics=['accuracy'])\n",
    "        During model training compile is an essential function that optimizes the model performace. It basically drives a computational graph that defines how the forward and backward passes will be executed during training. It sets a stage for optimization process by specifying how the model will learn from the data and udpate its parameters to make better predictions. The graph is also crutial for CPU/GPU utilization and automatic differenciation between predictions.\n",
    "        - loss function: is a mathematical function the qunatifies the difference between predicted and actual values in a model. Its measures the performace and guides the optimization process by providing feedback on how well it fits the data. 'BinaryCrossentropy' is a specific type of loss function used when dealing with binaey classification problems (where the output is a binary value like 0 to 1). It measures the difference between the predicted probabilities and actual binary labels.\n",
    "        - optimizer: is responsible for updating model weights (weight are connection management between two basic units within a neural network. In CNN these are the kernal/filters i.e the matrices that you use to perform convolution in a layer) to minimze the loss function. 'Adam' (Adaptive Movement Estimation) is an optimizer in deeplearning that dynamically adjusts the learning rate during training, which helps improve training speed.\n",
    "        - metrics: is used to monitor model performance during training. The 'accuracy' metric measures the propotion of corrected predicted instances out of the total instances in the dataset. It is a common metric for classification tasks.\n",
    "- 'summary' = print the summary of model architecture. This displays the output shape of each layer and the number of trainable parameters\n",
    "        model.summary()\n",
    "        During training keras provides us the summary() function to overview the archetecture of the neural network. It displays the model layers, their output shapes and number of trainable parameters in each layer. This information is crutial to understand the structure and complexity of the model.\n",
    "\n",
    "While adding additional CNN layers you will notice a linear increase in filter numbers. This is a common CNN archetecture choice known as pyramid. This model design strategy has been found effective for the follwing reason:\n",
    "- Hierarcial Feature Learning: As we move deeper into the network we increase the filters hence allowing the network to learn features at different levels of abstraction, just like a pyramid. Small filters capture global features whereas larger filters capture more finer details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-19 14:45:39.593121: E itex/core/wrapper/itex_gpu_wrapper.cc:49] Could not load Intel Extension for Tensorflow* GPU backend, GPU will not be used.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n",
      "2023-08-19 14:45:39.594539: E itex/core/wrapper/itex_gpu_wrapper.cc:49] Could not load Intel Extension for Tensorflow* GPU backend, GPU will not be used.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 4, 4, 2048)        23587712  \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 4, 4, 512)         9437696   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 2, 2, 512)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              8392704   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 22)                45078     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49853846 (190.18 MB)\n",
      "Trainable params: 26266134 (100.20 MB)\n",
      "Non-trainable params: 23587712 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-19 14:45:41.389137: E itex/core/wrapper/itex_gpu_wrapper.cc:49] Could not load Intel Extension for Tensorflow* GPU backend, GPU will not be used.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n"
     ]
    }
   ],
   "source": [
    "# Create the ResNet50 base model (Transfer learning - using an exiting trained model)\n",
    "# include_top is False because we dont want ResNet classfiying images to 1000 differenct classes. We will add our own classes\n",
    "# weights is imagenet is telling the layer to load weights that were trained on the ImageNet dataset. This will help our model to learn faster.\n",
    "base_model = ResNet50(input_shape=(128,128,3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Iterate over all layers and freeze the layers weights so they will not be updated during trainine\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create your custom model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add the ResNet50 base model\n",
    "model.add(base_model)\n",
    "\n",
    "# Add your existing layers\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dense(22, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='Adam', loss='BinaryCrossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-19 14:45:42.009361: E itex/core/wrapper/itex_gpu_wrapper.cc:49] Could not load Intel Extension for Tensorflow* GPU backend, GPU will not be used.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n",
      "2023-08-19 14:45:42.082529: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n",
      "2023-08-19 14:45:42.091010: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n",
      "2023-08-19 14:45:42.099511: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n",
      "2023-08-19 14:45:43.986936: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1088/1088 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.8250WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "1088/1088 [==============================] - 448s 409ms/step - loss: 0.0779 - accuracy: 0.8250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb9f856f700>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "     x_images_train_rgb,\n",
    "     y_labels_train,\n",
    "     batch_size=5,\n",
    "     callbacks=[\n",
    "         tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5),\n",
    "         tf.keras.callbacks.TensorBoard(log_dir=os.path.join(os.getcwd(), 'logs'), histogram_freq=1)\n",
    "     ],\n",
    "     epochs=1\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-19 14:53:09.772628: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n",
      "2023-08-19 14:53:09.776407: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n",
      "2023-08-19 14:53:09.779934: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n",
      "2023-08-19 14:53:10.231535: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 14s 449ms/step - loss: 0.0450 - accuracy: 0.8604\n",
      "loss:  0.04504675790667534\n",
      "accuracy:  0.8604166507720947\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_images_test_rgb, y_labels_test)\n",
    "\n",
    "print('loss: ', results[0])\n",
    "print('accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('body_part_classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-19 15:41:10.537548: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n",
      "2023-08-19 15:41:10.544523: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n",
      "2023-08-19 15:41:10.549046: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 128, 128, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m img \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(img_resized, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \n\u001b[1;32m     19\u001b[0m \u001b[39m# Make predictions using the loaded model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m prediction \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39;49mpredict(img)\n\u001b[1;32m     21\u001b[0m predicted_class \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(prediction)\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(prediction)\n",
      "File \u001b[0;32m~/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filen9rclljo.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "wrist_image_path = WRIST_IMAGES\n",
    "images_list = os.listdir(wrist_image_path)\n",
    "image_files = [image for image in images_list if image.lower().endswith(('.png'))][200:210]\n",
    "wrist_image_labels = []\n",
    "\n",
    "for image_file in image_files:\n",
    "    img_path = os.path.join(wrist_image_path, image_file)\n",
    "    img = plt.imread(img_path)\n",
    "\n",
    "    # Convert grayscale to RGB\n",
    "    if img.ndim == 2:\n",
    "        img = np.repeat(img[..., np.newaxis], 3, axis=-1) \n",
    "    \n",
    "    # Resize image input for the model and expand dimensions to create a batch of size 1\n",
    "    img_resized = tf.image.resize(img, (128, 128))\n",
    "    img = tf.expand_dims(img_resized, axis=0) \n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    prediction = loaded_model.predict(img)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    print(prediction)\n",
    "    print('Prediction class: ', predicted_class)\n",
    "    print('Prediction label: ', labels(int(predicted_class)))\n",
    "    \n",
    "    wrist_image_labels.append(labels(int(predicted_class)))\n",
    "\n",
    "total_images = len(wrist_image_labels)\n",
    "correctly_predicted = len([labels for labels in wrist_image_labels if 'Wrist' in labels])\n",
    "wrong_predicted_labels = [labels for labels in wrist_image_labels if 'Wrist' not in labels]\n",
    "accuracy_percentage = (correctly_predicted / total_images) * 100\n",
    "\n",
    "print('Total wrist images predicted:', total_images)\n",
    "print('Total wrist images correctly predicted:', correctly_predicted)\n",
    "print('Wrong predicted labels: ', wrong_predicted_labels)\n",
    "print('Percentage accuracy:', accuracy_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-19 14:53:30.207899: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-19 14:53:30.210507: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-19 14:53:30.243401: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-19 14:53:30.243778: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-19 14:53:30.965773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-19 14:53:31.505914: W itex/core/wrapper/itex_gpu_wrapper.cc:32] Could not load dynamic library: libmkl_sycl.so.3: cannot open shared object file: No such file or directory\n",
      "2023-08-19 14:53:31.528101: I itex/core/wrapper/itex_cpu_wrapper.cc:42] Intel Extension for Tensorflow* AVX512 CPU backend is loaded.\n",
      "2023-08-19 14:53:31.559169: W itex/core/ops/op_init.cc:58] Op: _QuantizedMaxPool3D is already registered in Tensorflow\n",
      "2023-08-19 14:53:31.569141: E itex/core/wrapper/itex_gpu_wrapper.cc:49] Could not load Intel Extension for Tensorflow* GPU backend, GPU will not be used.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n",
      "2023-08-19 14:53:31.569557: E itex/core/wrapper/itex_gpu_wrapper.cc:49] Could not load Intel Extension for Tensorflow* GPU backend, GPU will not be used.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.13.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@716.393] global loadsave.cpp:248 findDecoder imread_('/home/zaeem/Desktop/projects/jupyter_notebooks/body_part_classification/data/happy.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(DATA_DIR, \u001b[39m'\u001b[39m\u001b[39mhappy.png\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(img_path)\n\u001b[0;32m----> 3\u001b[0m img_resized \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mresize(img, (\u001b[39m128\u001b[39;49m,\u001b[39m128\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m img_resized \u001b[39m=\u001b[39m img_resized \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m\n\u001b[1;32m      5\u001b[0m plt\u001b[39m.\u001b[39mimshow(img_resized)\n",
      "File \u001b[0;32m~/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/projects/jupyter_notebooks/body_part_classification/.venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "img_path = os.path.join(DATA_DIR, 'happy.png')\n",
    "img = cv2.imread(img_path)\n",
    "img_resized = tf.image.resize(img, (128,128))\n",
    "img_resized = img_resized / 255.0\n",
    "plt.imshow(img_resized)\n",
    "img = tf.expand_dims(img_resized, axis=0)\n",
    "prediction = loaded_model.predict(img)\n",
    "print(labels(int(np.argmax(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
